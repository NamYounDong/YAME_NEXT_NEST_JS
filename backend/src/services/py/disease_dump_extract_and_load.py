#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‚˜ë¬´ìœ„í‚¤ ë¤í”„ í™•ë³´(ì—†ìœ¼ë©´ Hugging Faceì—ì„œ ìƒì„±) â†’ ë¯¸ë‹ˆ íŒŒì„œ â†’ (Aí˜•ì‹) MySQL ì ì¬ ë˜ëŠ” NDJSON ì¶œë ¥
- ë¤í”„ íŒŒì¼ì´ ì§€ì • ê²½ë¡œì— ì—†ìœ¼ë©´: Hugging Face `heegyu/namuwiki`(2022-03-01 ìŠ¤ëƒ…ìƒ·)ì—ì„œ
  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ `title,text`ë¥¼ ë½‘ì•„ JSONL.GZë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ê·¸ ë‹¤ìŒ, ë¤í”„ì—ì„œ '== ì¦ìƒ ==' ì„¹ì…˜ë§Œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ë¦¬ë‹¤ì´ë ‰íŠ¸(ë„˜ê²¨ì£¼ê¸°)ë¡œ ë™ì˜ì–´ë¥¼ ë¬¶ì–´
  (ì˜í•™ëª…=ìºë…¸ë‹ˆì»¬, ì¼ë°˜ëª…/ë™ì˜ì–´, ì¦ìƒ) ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
- **ì§ˆë³‘ ë¬¸ì„œ í•„í„°**ë¥¼ ì¶”ê°€í•˜ì—¬, ë¶„ë¥˜/ì„¹ì…˜/ì œëª© íŒíŠ¸ë¡œ **ì§ˆë³‘ ê´€ë ¨ í˜ì´ì§€ë§Œ** í†µê³¼ì‹œí‚µë‹ˆë‹¤.
- ì¶œë ¥ ëª¨ë“œ:
  1) --mode ndjson  : NDJSON(í•œ ì¤„ë‹¹ í•œ JSON) í‘œì¤€ì¶œë ¥
  2) --mode mysql   : MySQL í…Œì´ë¸”(Aí˜•ì‹: ë‹¨ì¼ í…Œì´ë¸” **NAMU_DISEASE_MASTER**)ì— ì—…ì„œíŠ¸ ì ì¬

í•„ìˆ˜/ê¶Œì¥ íŒ¨í‚¤ì§€:
  pip install datasets pymysql python-dotenv

í™˜ê²½ë³€ìˆ˜(ì„ íƒ):
  DB_HOST, DB_PORT, DB_USER, DB_PASS, DB_NAME

ì˜ˆì‹œ:
  # 1) ë¤í”„ ì—†ìœ¼ë©´ ìƒì„± â†’ NDJSON ì¶œë ¥
  python disease_dump_extract_and_load.py \
    --dump namuwiki_20220301.jsonl.gz --mode ndjson > disease_master.ndjson

  # 2) ë¤í”„ ì—†ìœ¼ë©´ ìƒì„± â†’ MySQL í…Œì´ë¸” ìƒì„± í›„ ì ì¬(ì—…ì„œíŠ¸)
  python disease_dump_extract_and_load.py \
    --dump namuwiki_20220301.jsonl.gz --mode mysql --create-table \
    --mysql-host localhost --mysql-port 3306 --mysql-user root --mysql-pass secret --mysql-db yame

ë¼ì´ì„ ìŠ¤ ìœ ì˜: ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œëŠ” CC BY-NC-SA 2.0 KR(ë¹„ì˜ë¦¬, ë™ì¼ì¡°ê±´)ì…ë‹ˆë‹¤.
"""

from __future__ import annotations
import argparse
import gzip
import io
import json
import os
import re
import sys
from collections import defaultdict
from typing import Dict, Iterator, List, Optional
from dotenv import load_dotenv

load_dotenv()  # .env ë¡œë“œ

# -----------------------------
# ë‚˜ë¬´ë§ˆí¬/í—¤ë”©/ë¦¬ë‹¤ì´ë ‰íŠ¸/ë¶„ë¥˜ ì •ê·œì‹
# -----------------------------
RE_REDIRECT = re.compile(
    r"^\s*#\s*(?:redirect|ë„˜ê²¨ì£¼ê¸°)\s*(?:\[\[)?\s*([^\]\n#]+?)\s*(?:\]\])?\s*$",
    re.IGNORECASE | re.MULTILINE,
)
RE_HEADING = re.compile(r"^(=+)\s*(.+?)\s*\1\s*$", re.MULTILINE)
RE_LINK_PIPED = re.compile(r"\[\[([^|\]]+)\|([^\]]+)\]\]")
RE_LINK_SIMPLE = re.compile(r"\[\[([^\]]+)\]\]")
RE_HTTP_LINK = re.compile(r"\[(?:https?|ftp)://[^\s\]]+\s+([^\]]+)\]")
RE_REF_TAG = re.compile(r"<ref[^>]*>.*?</ref>", re.DOTALL | re.IGNORECASE)
RE_TRIPLE_BRACE = re.compile(r"\{\{\{.*?\}\}\}", re.DOTALL)
RE_DOUBLE_BRACE = re.compile(r"\{\{.*?\}\}", re.DOTALL)
RE_BOLD_ITALIC = re.compile(r"''+")
RE_NOISY_LINES = re.compile(r"^\s*\[\[(?:ë¶„ë¥˜|ë¶„ë¥˜:).*\]\]\s*$", re.MULTILINE)
RE_CATEGORY = re.compile(r"\[\[\s*(?:ë¶„ë¥˜|category)\s*:\s*([^\]]+)\]\]", re.IGNORECASE)

SYMPTOM_KEYS = ["ì¦ìƒ", "ì„ìƒ ì¦ìƒ", "ì„ìƒì¦ìƒ", "ì£¼ìš” ì¦ìƒ", "ì¦í›„", "ì„ìƒ ì†Œê²¬", "ì¢…ë¥˜ ë° ì¦ìƒ"]
CAUSE_KEYS   = ["ì›ì¸", "ë³‘ì¸", "ë³‘íƒœìƒë¦¬", "ë³‘íƒœ ìƒë¦¬"]
TREAT_KEYS   = ["ì¹˜ë£Œ", "ì¹˜ë£Œë²•", "ì¹˜ë£Œ ë°©ë²•", "ì˜ˆí›„", "ê´€ë¦¬", "ì¹˜ë£Œì™€ ê´€ë¦¬"]

# í¬í•¨/ì œì™¸ ì¹´í…Œê³ ë¦¬ íŒíŠ¸(í•„ìš” ì‹œ ë³´ê°•)
INCLUDE_CATS = {
    "ì§ˆë³‘", "ê°ì—¼ë³‘", "ì „ì—¼ë³‘", "ì¦í›„êµ°", "ì˜í•™", "ì˜í•™ ìš©ì–´", "ì˜í•™ì  ìƒíƒœ",
    "ì •ì‹  ì§ˆí™˜", "í¬ê·€ì§ˆí™˜", "ì•”", "ì¢…ì–‘", "ë©´ì—­ì§ˆí™˜", "ìê°€ë©´ì—­ì§ˆí™˜", "ì§ˆí™˜", "ì¥ì• ", "ì—¼ì¦"
}
EXCLUDE_CATS = {
    "ì¸ë¬¼", "ì˜í™”", "ìŒì•…", "ê²Œì„", "ì• ë‹ˆë©”ì´ì…˜", "ì›¹íˆ°", "ê¸°ì—…", "íšŒì‚¬",
    "ì§€ëª…", "êµ­ê°€", "ì—­ì‚¬", "ìŠ¤í¬ì¸ ", "ì •ì¹˜", "êµ°ì‚¬", "ì†Œí”„íŠ¸ì›¨ì–´", "í”„ë¡œê·¸ë˜ë°",
}



def cat_contains_any_partial(categories: list[str], keywords: set[str]) -> bool:
    # ê³µë°± ì œê±° + ì†Œë¬¸ì ì •ê·œí™” í›„ ë¶€ë¶„ í¬í•¨ ê²€ì‚¬
    def norm(s: str) -> str:
        return re.sub(r"\s+", "", s).lower()
    cats_n = [norm(c) for c in categories]
    keys_n = [norm(k) for k in keywords]
    return any(any(k and (k in c) for k in keys_n) for c in cats_n)

# -----------------------------
# ìœ í‹¸: gzip íŒŒì¼ ì—´ê¸°
# -----------------------------
def open_maybe_gzip(path: str):
    if path.endswith('.gz'):
        return io.TextIOWrapper(gzip.open(path, 'rb'), encoding='utf-8')
    return open(path, 'r', encoding='utf-8')

# -----------------------------
# ì •ê·œì‹ ê¸°ë°˜ íŒŒì‹± ìœ í‹¸
# -----------------------------
def find_redirect_target(text: str) -> Optional[str]:
    m = RE_REDIRECT.search(text)
    return m.group(1).strip() if m else None

def canonical_of(title: str, redirects: Dict[str, str]) -> str:
    seen = set()
    cur = title
    while cur in redirects and cur not in seen:
        seen.add(cur)
        cur = redirects[cur]
    return cur

def slice_sections(text: str) -> List[tuple[int, str, int, int]]:
    out = []
    matches = list(RE_HEADING.finditer(text))
    for i, m in enumerate(matches):
        level = len(m.group(1))
        title = m.group(2).strip()
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        out.append((level, title, start, end))
    return out

def pick_section(text: str, keys: List[str]) -> Optional[str]:
    norm = lambda s: re.sub(r"\s+", "", s).lower()
    keyset = {norm(k) for k in keys}
    for level, title, s, e in slice_sections(text):
        if norm(title) in keyset:
            return text[s:e]
    return None

def clean_namumark(raw: str) -> str:
    if not raw:
        return ''
    t = raw
    t = RE_REF_TAG.sub('', t)
    t = RE_TRIPLE_BRACE.sub('', t)
    t = RE_DOUBLE_BRACE.sub('', t)
    t = RE_LINK_PIPED.sub(r'\2', t)
    t = RE_LINK_SIMPLE.sub(r'\1', t)
    t = RE_HTTP_LINK.sub(r'\1', t)
    t = RE_BOLD_ITALIC.sub('', t)
    t = RE_NOISY_LINES.sub('', t)
    t = t.replace('\t', ' ').replace('\r', '')
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

# -----------------------------
# ë¶„ë¥˜ ì¶”ì¶œ & ì§ˆë³‘ ë¬¸ì„œ íŒë³„
# -----------------------------
def extract_categories(text: str) -> List[str]:
    return [m.strip() for m in RE_CATEGORY.findall(text)]

def is_disease_page(title: str, text: str) -> bool:
    # 1) ì¹´í…Œê³ ë¦¬ íŒíŠ¸: ì§ˆë³‘ ê´€ë ¨ í¬í•¨ & ë¹„ì˜í•™ ì œì™¸
    cats_list = extract_categories(text)
    if cat_contains_any_partial(cats_list, INCLUDE_CATS) and not cat_contains_any_partial(cats_list, EXCLUDE_CATS):
        return True
    # 2) ì„¹ì…˜ êµ¬ì¡°: ì¦ìƒ + (ì›ì¸ or ì¹˜ë£Œ)
    has_sym = pick_section(text, SYMPTOM_KEYS) is not None
    has_ctx = (pick_section(text, CAUSE_KEYS) is not None) or (pick_section(text, TREAT_KEYS) is not None)
    # 3) ì œëª© íŒíŠ¸: â€¦ì§ˆí™˜/â€¦ë³‘/â€¦ì¦í›„êµ°/â€¦ì—¼/â€¦ì•”/â€¦ì¦
    title_hint = re.search(r"(ì§ˆí™˜|ë³‘|ì¦í›„êµ°|ì—¼|ì•”|ì¦)$", title)
    return bool(has_sym and (has_ctx or title_hint))

# -----------------------------
# ë¤í”„ íŒŒì¼ í™•ë³´(ì—†ìœ¼ë©´ Hugging Faceì—ì„œ ìƒì„±)
# -----------------------------
def ensure_dump_file(out_path: str, title_key: str = 'title', text_key: str = 'text', dataset: str = 'heegyu/namuwiki', split: str = 'train') -> None:
    """out_pathê°€ ì—†ìœ¼ë©´ Hugging Face datasetsë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì €ì¥(JSONL.GZ). ì´ë¯¸ ìˆìœ¼ë©´ no-op."""
    if os.path.exists(out_path):
        print(f"[ensure_dump] exists: {out_path}", file=sys.stderr)
        return
    print(f"[ensure_dump] creating from Hugging Face: {dataset} â†’ {out_path}", file=sys.stderr)
    try:
        from datasets import load_dataset
    except Exception as e:
        raise RuntimeError("datasets íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install datasets") from e

    ds = load_dataset(dataset, split=split, streaming=True)
    with gzip.open(out_path, 'wt', encoding='utf-8') as fw:
        for ex in ds:
            title = ex.get(title_key)
            text = ex.get(text_key)
            if not title or not text:
                continue
            line = json.dumps({"title": title, "text": text}, ensure_ascii=False)
            fw.write(line + "\n")
    print(f"[ensure_dump] done: {out_path}", file=sys.stderr)

# -----------------------------
# 1íŒ¨ìŠ¤: ë¦¬ë‹¤ì´ë ‰íŠ¸ ë§µ ìƒì„±
# -----------------------------
def pass1_build_redirects(dump_path: str, title_key: str = 'title', text_key: str = 'text') -> Dict[str, str]:
    redirects: Dict[str, str] = {}
    with open_maybe_gzip(dump_path) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            title = str(obj.get(title_key, '')).strip()
            text = str(obj.get(text_key, '')).strip()
            if not title or not text:
                continue
            tgt = find_redirect_target(text)
            if tgt:
                redirects[title] = tgt
    return redirects

# -----------------------------
# 2íŒ¨ìŠ¤: ë ˆì½”ë“œ ìƒì„± ì œë„ˆë ˆì´í„° (NDJSON ì†ŒìŠ¤)
# -----------------------------
def generate_records(dump_path: str, title_key: str = 'title', text_key: str = 'text') -> Iterator[dict]:
    redirects = pass1_build_redirects(dump_path, title_key, text_key)

    inverse: Dict[str, set] = defaultdict(set)
    for src, tgt in redirects.items():
        inverse[canonical_of(tgt, redirects)].add(src)

    symptoms: Dict[str, str] = {}
    counts: Dict[str, int] = defaultdict(int)
    aliases: Dict[str, set] = defaultdict(set)

    with open_maybe_gzip(dump_path) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            title = str(obj.get(title_key, '')).strip()
            text  = str(obj.get(text_key, '')).strip()
            if not title or not text:
                continue

            # ğŸ” ì§ˆë³‘ ë¬¸ì„œë§Œ í†µê³¼
            if not is_disease_page(title, text):
                continue

            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ë¬¸ì„œ(ë³¸ë¬¸X)ëŠ” ìŠ¤í‚µ (ë™ì˜ì–´ëŠ” inverseë¡œ ì²˜ë¦¬ë¨)
            if find_redirect_target(text):
                continue

            canon = canonical_of(title, redirects)
            counts[canon] += 1
            if canon in inverse:
                aliases[canon].update(inverse[canon])

            raw = pick_section(text, SYMPTOM_KEYS)
            if raw and canon not in symptoms:
                clean = clean_namumark(raw)
                if clean:
                    symptoms[canon] = clean

    snapshot = guess_snapshot_from_filename(dump_path)
    provider = 'namuwiki'
    keys = sorted(set(counts) | set(symptoms) | set(aliases))
    for canon in keys:
        yield {
            'canonical_title': canon,
            'aliases': sorted(aliases.get(canon, set())),
            'symptom_text': symptoms.get(canon, ''),
            'source_titles_count': int(counts.get(canon, 0)),
            'source': {
                'provider': provider,
                'snapshot': snapshot,
            },
        }

# -----------------------------
# ìŠ¤ëƒ…ìƒ· ì¶”ì •(íŒŒì¼ëª…ì— ë‚ ì§œ í¬í•¨ ì‹œ)
# -----------------------------
def guess_snapshot_from_filename(path: str) -> Optional[str]:
    base = os.path.basename(path)
    m = re.search(r'(\d{8}|\d{6})', base)
    if not m:
        return None
    val = m.group(1)
    if len(val) == 6:
        return '20' + val  # yymmdd â†’ 20yymmdd ê°€ì •
    return val

# -----------------------------
# NDJSON ì¶œë ¥
# -----------------------------
def emit_ndjson(dump_path: str) -> None:
    for rec in generate_records(dump_path):
        print(json.dumps(rec, ensure_ascii=False))

# -----------------------------
# MySQL ì ì¬(Aí˜•ì‹ ë‹¨ì¼ í…Œì´ë¸”: NAMU_DISEASE_MASTER)
# -----------------------------
MYSQL_DDL_A = r"""
CREATE TABLE IF NOT EXISTS NAMU_DISEASE_MASTER (
  ID                   BIGINT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
  CANONICAL_TITLE      VARCHAR(255) NOT NULL COMMENT 'ì˜í•™ì  ì§ˆë³‘ ëª…ì¹­(ì •ì‹/ìºë…¸ë‹ˆì»¬)',
  COMMON_NAME          VARCHAR(255) NULL COMMENT 'ì¼ë°˜ì  ì§ˆë³‘ëª…(ëŒ€í‘œ 1ê°œ)',
  ALIASES_JSON         JSON NULL COMMENT 'ë™ì˜ì–´/ì¼ë°˜ëª… ë°°ì—´(JSON)',
  SYMPTOM_TEXT         MEDIUMTEXT NULL COMMENT 'ì¦ìƒ ì„¹ì…˜ í‰ë¬¸í™”',
  SOURCE_PROVIDER      VARCHAR(50) NOT NULL DEFAULT 'namuwiki',
  SOURCE_SNAPSHOT      VARCHAR(20) NULL,
  SOURCE_TITLES_COUNT  INT NOT NULL DEFAULT 0,
  CREATED_AT           TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  UPDATED_AT           TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY UK_CANONICAL (CANONICAL_TITLE),
  FULLTEXT KEY FT_SYMPTOM (SYMPTOM_TEXT)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
"""


def mysql_connect(host: str, port: int, user: str, password: str, db: str):
    try:
        import pymysql  # type: ignore
    except Exception as e:
        raise RuntimeError("pymysql íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install pymysql") from e
    return pymysql.connect(
        host=host,
        port=port,
        user=user,
        password=password,
        database=db,
        charset='utf8mb4',
        autocommit=False,
    )


def ensure_table(conn) -> None:
    with conn.cursor() as cur:
        cur.execute(MYSQL_DDL_A)
    conn.commit()


def insert_mysql(
    dump_path: str,
    host: str,
    port: int,
    user: str,
    password: str,
    db: str,
    create_table: bool = False,
    batch_size: int = 500,
) -> int:
    conn = mysql_connect(host, port, user, password, db)
    try:
        if create_table:
            ensure_table(conn)

        total = 0
        batch: List[dict] = []

        def flush_batch():
            nonlocal total, batch
            if not batch:
                return
            placeholders = ','.join(['(?,?,?,?,?,?,?)'] * len(batch))
            sql = (
                "INSERT INTO NAMU_DISEASE_MASTER "
                "(CANONICAL_TITLE, COMMON_NAME, ALIASES_JSON, SYMPTOM_TEXT, SOURCE_PROVIDER, SOURCE_SNAPSHOT, SOURCE_TITLES_COUNT) "
                f"VALUES {placeholders} "
                "ON DUPLICATE KEY UPDATE "
                "COMMON_NAME=VALUES(COMMON_NAME), "
                "ALIASES_JSON=VALUES(ALIASES_JSON), "
                "SYMPTOM_TEXT=VALUES(SYMPTOM_TEXT), "
                "SOURCE_PROVIDER=VALUES(SOURCE_PROVIDER), "
                "SOURCE_SNAPSHOT=VALUES(SOURCE_SNAPSHOT), "
                "SOURCE_TITLES_COUNT=VALUES(SOURCE_TITLES_COUNT), "
                "UPDATED_AT=CURRENT_TIMESTAMP"
            )
            params: List[object] = []
            for r in batch:
                aliases = r.get('aliases') or []
                common = aliases[0] if aliases else None
                params.extend([
                    r.get('canonical_title'),
                    common,
                    json.dumps(aliases, ensure_ascii=False),
                    r.get('symptom_text') or None,
                    (r.get('source') or {}).get('provider') or 'namuwiki',
                    (r.get('source') or {}).get('snapshot'),
                    int(r.get('source_titles_count') or 0),
                ])
            with conn.cursor() as cur:
                # PyMySQLëŠ” %s í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš© â†’ '?' ì¹˜í™˜
                cur.execute('delete from namu_disease_master')
                sql_pymysql = sql.replace('?', '%s')
                cur.execute(sql_pymysql, params)
            conn.commit()
            total += len(batch)
            batch = []

        for rec in generate_records(dump_path):
            batch.append(rec)
            if len(batch) >= batch_size:
                flush_batch()
        flush_batch()
        return total
    finally:
        conn.close()

# -----------------------------
# ë©”ì¸
# -----------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--dump', default='namuwiki_20220301.jsonl.gz', help='ë¤í”„ ê²½ë¡œ(JSONL.GZ). ì—†ìœ¼ë©´ ìë™ ìƒì„±')
    ap.add_argument('--mode', choices=['ndjson', 'mysql'], default='ndjson', help='ì¶œë ¥ ëª¨ë“œ')
    ap.add_argument('--title-key', default='title')
    ap.add_argument('--text-key', default='text')
    ap.add_argument('--no-ensure-dump', action='store_true', help='ë¤í”„ ìë™ ìƒì„± ê±´ë„ˆë›°ê¸°')

    # MySQL ì˜µì…˜
    ap.add_argument('--mysql-host', default=os.getenv('DB_HOST', 'localhost'))
    ap.add_argument('--mysql-port', type=int, default=int(os.getenv('DB_PORT', '3306')))
    ap.add_argument('--mysql-user', default=os.getenv('DB_USER', 'root'))
    ap.add_argument('--mysql-pass', default=os.getenv('DB_PASS', ''))
    ap.add_argument('--mysql-db',   default=os.getenv('DB_NAME', 'yame'))
    ap.add_argument('--create-table', action='store_true', help='NAMU_DISEASE_MASTER í…Œì´ë¸” ìƒì„±')
    ap.add_argument('--batch-size', type=int, default=500)

    args = ap.parse_args()

    # 0) ë¤í”„ ë³´ì¥
    if not args.no_ensure_dump:
        ensure_dump_file(args.dump, title_key=args.title_key, text_key=args.text_key)

    # 1) ëª¨ë“œ ë¶„ê¸°
    if args.mode == 'ndjson':
        emit_ndjson(args.dump)
    else:
        total = insert_mysql(
            dump_path=args.dump,
            host=args.mysql_host,
            port=args.mysql_port,
            user=args.mysql_user,
            password=args.mysql_pass,
            db=args.mysql_db,
            create_table=args.create_table,
            batch_size=args.batch_size,
        )
        print(json.dumps({'ok': True, 'inserted_or_upserted': total}, ensure_ascii=False))


if __name__ == '__main__':
    main()
